{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries"
      ],
      "metadata": {
        "id": "CRBQAXSBhudQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tt3-ukdohrvn"
      },
      "outputs": [],
      "source": [
        "!pip install -q gymnasium\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the Environment"
      ],
      "metadata": {
        "id": "1KuWHYMG-tnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "\n",
        "state_space = env.observation_space.n\n",
        "action_space = env.action_space.n"
      ],
      "metadata": {
        "id": "MO6QVbAPhuDQ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Q-Table"
      ],
      "metadata": {
        "id": "lKSrZvdq-x0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.zeros((state_space, action_space))\n"
      ],
      "metadata": {
        "id": "ZIbHE-Lsh2kB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "lpjEabAZ-0Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1      # Learning rate\n",
        "gamma = 0.99     # Discount factor\n",
        "epsilon = 1.0    # Exploration rate\n",
        "epsilon_decay = 0.001\n",
        "episodes = 2000\n"
      ],
      "metadata": {
        "id": "DxQSj__sh4IJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q-Learning Algorithm (Off-Policy)"
      ],
      "metadata": {
        "id": "9MiBRtjriBD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        q_table[state, action] = q_table[state, action] + alpha * (\n",
        "            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]\n",
        "        )\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    epsilon = max(0.01, epsilon - epsilon_decay)\n"
      ],
      "metadata": {
        "id": "VqGg803gh6Ih"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "Uses best possible future action\n",
        "\n",
        "Learns optimal policy even if agent behaves randomly"
      ],
      "metadata": {
        "id": "crIukOHeiQvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-initialize Q-Table"
      ],
      "metadata": {
        "id": "dBMURLddiXwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_table_sarsa = np.zeros((state_space, action_space))\n",
        "epsilon = 1.0"
      ],
      "metadata": {
        "id": "gmwD_60oh7nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SARSA Algorithm (On-Policy)"
      ],
      "metadata": {
        "id": "BDqQoYdgifvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(episodes):\n",
        "    state, info = env.reset()\n",
        "\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = np.argmax(q_table_sarsa[state])\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        next_state, reward, terminated, truncated, info = env.step(action);\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            next_action = env.action_space.sample()\n",
        "        else:\n",
        "            next_action = np.argmax(q_table_sarsa[next_state])\n",
        "\n",
        "        # SARSA update rule\n",
        "        q_table_sarsa[state, action] = q_table_sarsa[state, action] + alpha * (\n",
        "            reward + gamma * q_table_sarsa[next_state, next_action] - q_table_sarsa[state, action]\n",
        "        )\n",
        "\n",
        "        state = next_state\n",
        "        action = next_action\n",
        "\n",
        "    epsilon = max(0.01, epsilon - epsilon_decay)"
      ],
      "metadata": {
        "id": "NdpLbye1icaW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q-Learning Q-table:\\n\", q_table)\n",
        "print(\"\\nSARSA Q-table:\\n\", q_table_sarsa)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2auGWigIij1e",
        "outputId": "c9dad192-ab36-4549-8e5f-c92721de1654"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Learning Q-table:\n",
            " [[0.50137894 0.47631036 0.48421145 0.45603591]\n",
            " [0.09186902 0.0570871  0.05250653 0.46739552]\n",
            " [0.38409074 0.15391945 0.05165008 0.0567021 ]\n",
            " [0.07730967 0.00321427 0.00194719 0.00417069]\n",
            " [0.51441131 0.30522337 0.43469294 0.42085162]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.08754964 0.0864306  0.27236785 0.06742032]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.30334063 0.4067126  0.3133837  0.54649424]\n",
            " [0.38099236 0.63555867 0.32188575 0.19260714]\n",
            " [0.66838505 0.33492117 0.26342163 0.13309709]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.27351325 0.34089321 0.71930426 0.33236988]\n",
            " [0.49774639 0.630934   0.84800359 0.51917434]\n",
            " [0.         0.         0.         0.        ]]\n",
            "\n",
            "SARSA Q-table:\n",
            " [[0.46307269 0.41064049 0.4211268  0.4034234 ]\n",
            " [0.0229756  0.09611246 0.0466548  0.32631377]\n",
            " [0.05218288 0.22007996 0.05674719 0.06499841]\n",
            " [0.03291066 0.10229502 0.00102356 0.00499221]\n",
            " [0.48519021 0.40301696 0.3738459  0.31510609]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.08315211 0.0838395  0.26186201 0.02648437]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.28105808 0.29611701 0.29755872 0.51486816]\n",
            " [0.44703138 0.58203657 0.49945125 0.31321774]\n",
            " [0.51252888 0.28454423 0.24959227 0.13344902]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.43723935 0.51558765 0.73906158 0.46332723]\n",
            " [0.61164192 0.79837392 0.66740951 0.68646816]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature       | Q-Learning  | SARSA         |\n",
        "| ------------- | ----------- | ------------- |\n",
        "| Type          | Off-policy  | On-policy     |\n",
        "| Future Action | Max Q-value | Chosen action |\n",
        "| Risk          | Aggressive  | Conservative  |\n",
        "| Convergence   | Faster      | Safer         |\n"
      ],
      "metadata": {
        "id": "ig3EFBda-VC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this experiment, Q-Learning and SARSA were implemented using the FrozenLake environment. Q-Learning, being an off-policy algorithm, learned the optimal policy faster by always considering the best possible future action. SARSA, on the other hand, followed an on-policy approach and learned more conservatively by updating values based on the actions actually taken. While Q-Learning showed faster convergence, SARSA demonstrated safer learning behavior. Both algorithms successfully learned optimal policies, highlighting the strengths of reinforcement learning techniques.     \n"
      ],
      "metadata": {
        "id": "RIdAPtPA-ZZt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WtKkyxS6-SBm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}